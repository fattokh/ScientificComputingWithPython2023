{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. **PCA on 3D dataset**\n",
    "\n",
    "* Generate a dataset simulating 3 features, each with N entries (N being ${\\cal O}(1000)$). Each feature is made by random numbers generated according the normal distribution $N(\\mu,\\sigma)$ with mean $\\mu_i$ and standard deviation $\\sigma_i$, with $i=1, 2, 3$. Generate the 3 variables $x_{i}$ such that:\n",
    "    * $x_1$ is distributed as $N(0,1)$\n",
    "    * $x_2$ is distributed as $x_1+N(0,3)$\n",
    "    * $x_3$ is given by $2x_1+x_2$\n",
    "* Find the eigenvectors and eigenvalues using the eigendecomposition of the covariance matrix\n",
    "* Find the eigenvectors and eigenvalues using the SVD. Check that the two procedures yield to same result\n",
    "* What percent of the total dataset's variability is explained by the principal components? Given how the dataset was constructed, do these make sense? Reduce the dimensionality of the system so that at least 99% of the total variability is retained\n",
    "* Redefine the data according to the new basis from the PCA\n",
    "* Plot the data, in both the original and the new basis. The figure should have 2 rows (the original and the new basis) and 3 columns (the $[x_0, x_1]$, $[x_0, x_2]$ and $[x_1, x_2]$ projections) of scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (3, 1000)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'la' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m cov \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcov(X)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#print(cov)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Compute using eignevalues\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m l, V \u001b[38;5;241m=\u001b[39m \u001b[43mla\u001b[49m\u001b[38;5;241m.\u001b[39meig(cov)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(l)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(V)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'la' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = 1000 # size\n",
    "mu1, sigma1 = 0, 1# mean and standard deviation\n",
    "x1 = np.random.normal(mu1, sigma1, N)\n",
    "mu2, sigma2 = 0, 3# mean and standard deviation\n",
    "x2 = x1 + np.random.normal(mu2, sigma2, N)\n",
    "x3 = 2*x1 + x2\n",
    "# create a matrix with the three vectors\n",
    "X = np.array([x1,x2, x3])\n",
    "print(\"X:\", X.shape)\n",
    "#Find the covariance matrix from the samples\n",
    "cov = np.cov(X)\n",
    "#print(cov)\n",
    "# Compute using eignevalues\n",
    "l, V = la.eig(cov)\n",
    "print(l)\n",
    "print(V)\n",
    "# Compute using svd\n",
    "U, s, Vt = la.svd(cov)\n",
    "# Estimate eigenvalues\n",
    "l_est = np.matmul(s, (np.matmul(Vt ,U))) # Can be proven\n",
    "print(\"l_est:\",l_est)\n",
    "print(U)\n",
    "# As we can find:\n",
    "# V = U\n",
    "# l = s*Vt*U\n",
    "\n",
    "# \n",
    "l = abs(l)\n",
    "l_ord = np.sort(l)\n",
    "print(l_ord)\n",
    "variability = abs((l_ord[2]+l_ord[1])/l_ord.sum())\n",
    "print(variability)\n",
    "#That's make sense due to the enormous difference between values\n",
    "# We just need to delete the 1 eigenvector, the once associated to the smallest eigenvalue\n",
    "\n",
    "toDel = np.argmin(l)\n",
    "V_mod = np.copy(V)\n",
    "V_mod[:3, toDel] = 0\n",
    "out = np.dot(X.T, V_mod)\n",
    "X = np.dot(X.T, V)\n",
    "\n",
    "x1 = X[:, 0]\n",
    "x2 = X[:, 1]\n",
    "x3 = X[:, 2]\n",
    "\n",
    "o1 = out[:, 0]\n",
    "o2 = out[:, 1]\n",
    "o3 = out[:, 2]\n",
    "\n",
    "# Print\n",
    "fig, axs= plt.subplots(2, 3,figsize=(15, 15))\n",
    "axs[0, 0].scatter(x1, x2)\n",
    "axs[0, 0].set_title(\"x_0 vs x_1 original eigenvector basis\")\n",
    "axs[0, 0].set_xlabel(\"x_0\")\n",
    "axs[0, 0].set_ylabel(\"x_1\")\n",
    "axs[0, 1].scatter(x1, x3)\n",
    "axs[0, 1].set_title(\"x_0 vs x_2 original eigenvector basis\")\n",
    "axs[0, 1].set_xlabel(\"x_0\")\n",
    "axs[0, 1].set_ylabel(\"x_2\")\n",
    "axs[0, 2].scatter(x2, x3)\n",
    "axs[0, 2].set_title(\"x_1 vs x_2 original eigenvector basis\")\n",
    "axs[0, 2].set_xlabel(\"x_1\")\n",
    "axs[0, 2].set_ylabel(\"x2\")\n",
    "\n",
    "axs[1, 0].scatter(o1, o2)\n",
    "axs[1, 0].set_title(\"x_0 vs x_1 new basis\")\n",
    "axs[1, 0].set_xlabel(\"x_0\")\n",
    "axs[1, 0].set_ylabel(\"x_1\")\n",
    "axs[1, 1].scatter(o1, o3)\n",
    "axs[1, 1].set_title(\"x_0 vs x_2 new basis\")\n",
    "axs[1, 1].set_xlabel(\"x_0\")\n",
    "axs[1, 1].set_ylabel(\"x_2\")\n",
    "axs[1, 2].scatter(o2, o3)\n",
    "axs[1, 2].set_title(\"x_1 vs x_2 new basis\")\n",
    "axs[1, 2].set_xlabel(\"x_1\")\n",
    "axs[1, 2].set_ylabel(\"x_2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2\\. **PCA on a nD dataset**\n",
    "\n",
    "* Start from the dataset you have genereted in the previous exercise and add uncorrelated random noise. Such noise should be represented by other 10 uncorrelated variables normally distributed, with a standard deviation much smaller (e.g. a factor 20) than those used to generate the $x_1$ and $x_2$. Repeat the PCA procedure and compare the results with what you have obtained before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = (1000, 10) # size\n",
    "mu1N, sigma1N = mu1, sigma1/20# mean and standard deviation\n",
    "w1 = np.random.normal(mu1N, sigma1N, N)\n",
    "w1 = w1.sum(axis = 1)\n",
    "x1 = x1 + w1\n",
    "\n",
    "N = (1000, 10) # size\n",
    "mu2N, sigma2N = mu2, sigma2/20# mean and standard deviation\n",
    "w2 = np.random.normal(mu2N, sigma2N, N)\n",
    "w2 = w2.sum(axis = 1)\n",
    "x2 = x2 + w2\n",
    "\n",
    "x3 = 2*x1 + x2\n",
    "\n",
    "# Now repeat the code presented before\n",
    "X = np.array([x1,x2, x3])\n",
    "cov = np.cov(X)\n",
    "l, V = la.eig(cov)\n",
    "\n",
    "l = abs(l)\n",
    "l_ord = np.sort(l)\n",
    "variability = abs((l_ord[2]+l_ord[1])/l_ord.sum())\n",
    "print(\"The Variability is: \", variability)\n",
    "\n",
    "toDel = np.argmin(l)\n",
    "V_mod = np.copy(V)\n",
    "V_mod[:3, toDel] = 0\n",
    "out = np.dot(X.T, V_mod)\n",
    "X = np.dot(X.T, V)\n",
    "\n",
    "x1 = X[:, 0]\n",
    "x2 = X[:, 1]\n",
    "x3 = X[:, 2]\n",
    "\n",
    "o1 = out[:, 0]\n",
    "o2 = out[:, 1]\n",
    "o3 = out[:, 2]\n",
    "\n",
    "\n",
    "fig, axs= plt.subplots(2, 3,figsize=(15, 15))\n",
    "axs[0, 0].scatter(x1, x2)\n",
    "axs[0, 0].set_title(\"x0 vs x1 original eigenvector basis\")\n",
    "axs[0, 0].set_xlabel(\"x0\")\n",
    "axs[0, 0].set_ylabel(\"x1\")\n",
    "axs[0, 1].scatter(x1, x3)\n",
    "axs[0, 1].set_title(\"x0 vs x2 original eigenvector basis\")\n",
    "axs[0, 1].set_xlabel(\"x0\")\n",
    "axs[0, 1].set_ylabel(\"x2\")\n",
    "axs[0, 2].scatter(x2, x3)\n",
    "axs[0, 2].set_title(\"x1 vs x2 original eigenvector basis\")\n",
    "axs[0, 2].set_xlabel(\"x1\")\n",
    "axs[0, 2].set_ylabel(\"x2\")\n",
    "\n",
    "axs[1, 0].scatter(o1, o2)\n",
    "axs[1, 0].set_title(\"x0 vs x1 new basis\")\n",
    "axs[1, 0].set_xlabel(\"x0\")\n",
    "axs[1, 0].set_ylabel(\"x1\")\n",
    "axs[1, 1].scatter(o1, o3)\n",
    "axs[1, 1].set_title(\"x0 vs x2 new basis\")\n",
    "axs[1, 1].set_xlabel(\"x0\")\n",
    "axs[1, 1].set_ylabel(\"x2\")\n",
    "axs[1, 2].scatter(o2, o3)\n",
    "axs[1, 2].set_title(\"x1 vs x2 new basis\")\n",
    "axs[1, 2].set_xlabel(\"x1\")\n",
    "axs[1, 2].set_ylabel(\"x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. **Optional**: **PCA on the MAGIC dataset**\n",
    "\n",
    "Perform a PCA on the magic04.data dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset and its description on the proper data directory\n",
    "#!wget https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data -P data/\n",
    "#!wget https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.names -P data/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/magic04.data\", header=None)\n",
    "data = data.drop(columns=[10])\n",
    "display(data)\n",
    "\n",
    "# SVD decomposition on datas\n",
    "U, S, Vt = np.linalg.svd(data)\n",
    "\n",
    "# Eigenvalues\n",
    "l = S**2/(-1) \n",
    "V = U\n",
    "print(\"\\nThe eigenvalues are:\\n\", l)\n",
    "print(\"\\nThe eigenvectors are:\\n\", V)\n",
    "\n",
    "#Perform PCA\n",
    "l_sum = l.sum()\n",
    "\n",
    "for i in range(magic.shape[1]):\n",
    "    print(\"By selecting the component %d, we retain %.2f%% of the total variability\" % (i,((l[i]/l_sum)*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
